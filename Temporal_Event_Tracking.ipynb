{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Event Tracking of events in news\n",
    " - Source: https://towardsdatascience.com/natural-language-processing-event-extraction-f20d634661d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet \n",
    "import contractions\n",
    "contractions.add(\"Here's\", 'Here is')\n",
    "contractions.add(\"Inc.\", \"Inc\")\n",
    "\n",
    "import nlpre\n",
    "from nlpre import titlecaps, dedash, identify_parenthetical_phrases\n",
    "from nlpre import replace_acronyms, replace_from_dictionary,url_replacement\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "newStopWords = ['.','?','%','google','Wells Fargo','let','us','got','year','thing','would','make','time','Donald Trump','Charles Schwab','Morgan Stanley','Credit Suisse','Reuters','Bank of America','Guggenheim','Deutsch Bank','Goldman Sachs','Facebook','Fifth Third Bank','New York','Washington','Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday','January','February','March','April','May','June','July','August','September','October','November','December','from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come','the']\n",
    "stopwords.extend(newStopWords)\n",
    "stemmer = nltk.stem.snowball.SnowballStemmer('english')\n",
    "stop_list=set(stopwords)\n",
    "url_replacer = url_replacement()\n",
    "\n",
    "pd.set_option('max_colwidth',-1)\n",
    "\n",
    "#replace newline and multi-spaces \n",
    "def clean_text_str(text):\n",
    "    text = text.replace(\"\\n\",\" \").replace(\"\\t\",\" \").replace(\"\\r\",\" \")\n",
    "    text = re.sub(r\" +\",\" \",text)\n",
    "    text = text.strip()\n",
    "    return text \n",
    "\n",
    "#clean unicode text \n",
    "def clean_unicode_text(text):\n",
    "    str_test = clean_text_str(str(text))\n",
    "    new_str =unicodedata.normalize(\"NFKD\",str_test)\n",
    "    return new_str \n",
    "\n",
    "#remove all URL references\n",
    "def remove_URL(sample):\n",
    "    \"\"\"Remove URLs from a sample string\"\"\"\n",
    "    #looks for any urls with spaces between domain.s\\{1+}com\n",
    "    fixspaces = re.sub(r\"(?<=\\.)\\s+(?=com)\",r'',sample)\n",
    "    \n",
    "    #remove https, remove wwws, and then loook for suffix's tied to websites\n",
    "    return  re.sub(r\"[^\\s]*\\.(com|org|net|gov|edu)\\S*\", r'', re.sub(r\"www//.S+\", \"\",re.sub(r\"http\\S+\", \"\", sample)))\n",
    "\n",
    "#remove email references\n",
    "def remove_emails(text):\n",
    "    return re.sub(r\"\\S*@\\S*\\s?\",r'',text)\n",
    "\n",
    "#extract phrases of text found in parenthesis\n",
    "def extract_parentheses_text(text):\n",
    "    return re.findall(r'\\([^)]*\\)',text, re.MULTILINE)\n",
    "\n",
    "#remove the text found in parentheses\n",
    "def remove_parentheses_txt(text,parentheses_txt_list):\n",
    "    for val in parentheses_txt_list:\n",
    "        text = text.replace(val,'')\n",
    "    return text \n",
    "\n",
    "#cap case all full UPPER case abreviations and phrases in a pandas series that are not contained in the list of stopwords \n",
    "def capcase_abrevs(p_series,stop_list):\n",
    "    out = p_series.apply(lambda x:  ' '.join([word.title()  if (word.isupper() and word.lower() not in stop_list) else \n",
    "                                               word.lower() if word.lower() in stop_list else \n",
    "                                               word for word in str(x).split() ]))\n",
    "    return out \n",
    "\n",
    "#replace yall with you all , here's with here is , etc...\n",
    "def replace_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "#remove multiple full stops  and add space after end of sentences\n",
    "def remove_multiple_periods(s):\n",
    "    instr= re.sub(r'\\.+', \".\", s) #replace multiple periods \n",
    "    return  re.sub(r\"\\.+(?! )\", \". \",instr) #insert space between sentences and periods \n",
    "\n",
    "#remove repeated words within a sentence\n",
    "def remove_repeatwords(sentence):\n",
    "    re_output = re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1',sentence)\n",
    "    return re_output\n",
    "\n",
    "#func to compare original with processed string \n",
    "def get_string_diff(str1,str2):\n",
    "    import difflib\n",
    "    for line in difflib.context_diff(str1,str2):\n",
    "        print(line)\n",
    "\n",
    "def generate_stopphrases(stop_word_list):\n",
    "    stop_phrases = []\n",
    "    for item in stop_word_list:\n",
    "        if len(item.split())>1:\n",
    "            stop_phrases.append(' '.join([word.lower() for word in item.split()]))\n",
    "            return stop_phrases\n",
    "        \n",
    "def replace_stopphrases(doc,stop_phrase_lst,replace_val = ''):\n",
    "    for item in stop_phrase_lst:\n",
    "        redata = re.compile(re.escape(item),re.IGNORECASE)\n",
    "        doc = redata.sub(replace_val,doc)\n",
    "    return doc \n",
    "\n",
    "import gensim \n",
    "def incorp_phrases(sent_word_tokenized_docs, min_count = 5, threshold=.9):\n",
    "    word_tokenized_docs = [[item for sublist in doc for item in sublist] for doc in sent_word_tokenized_docs]\n",
    "    phrases = gensim.models.phrases.Phrases(word_tokenized_docs, min_count=min_count, threshold = threshold, scoring = 'npmi')\n",
    "    bigrams = gensim.models.phrases.Phraser(phrases)\n",
    "    out_biphrased = bigrams[word_tokenized_docs]\n",
    "    phrases2 = gensim.models.phrases.Phrases(out_biphrased, min_count=4, threshold = threshold, scoring = 'npmi')\n",
    "    trigrams = gensim.models.phrases.Phraser(phrases2)\n",
    "    out_trigrams = [[trigrams[word_toks] for word_toks in sent] for sent in sent_word_tokenized_docs]\n",
    "    phrase_dict  = {phrase:score for phrase,score in phrases2.export_phrases(word_tokenized_docs)}\n",
    "    return out_trigrams, phrase_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Level Preprocssing Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.sentence tokenize data \n",
    "#spacy.load('en_core_web_sm')\n",
    "#python -m nltk.downloader all\n",
    "#nltk.download() #C:\\nltk_data\n",
    "\n",
    "import spacy\n",
    "import nltk \n",
    "import string \n",
    "import itertools\n",
    "from collections import defaultdict \n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser'])\n",
    "word_tokenizer = nltk.tokenize.word_tokenize\n",
    "sent_tokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n",
    "\n",
    "def tokenize_sent(doc_str):\n",
    "    return sent_tokenizer.tokenize(doc_str)\n",
    "\n",
    "def tokenize_words(sent):\n",
    "    return word_tokenizer(sent)\n",
    "\n",
    "def lemma_sent(nlp, sentence,allowed_postags=None):\n",
    "    doc = nlp(sentence)\n",
    "    if allowed_postags:\n",
    "        return [token.lemma_ for token in doc if token.pos_ in allowed_postags]\n",
    "    else: \n",
    "        return [token.lemma_ for token in doc]       \n",
    "\n",
    "def pos_sent(nlp,sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return [(token.text,token.pos_) for token in doc]\n",
    "\n",
    "def ner_sent(nlp,sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return [(token.text,token.label_) for token in doc.ents]\n",
    "\n",
    "def get_noun_chunks(nlp, sentence):\n",
    "    doc = self.nlp(sentence)\n",
    "    return [(chunk.text.chunk.root.text) for chunk in doc.noun_chunks]\n",
    "\n",
    "def replace_stopwords(word_tokenized_sent, stop_words=None):\n",
    "    return [word for word in word_tokenized_sent if str(word).lower() not in stop_words]\n",
    "\n",
    "\n",
    "#map each original word to the most common stem \n",
    "def map_stems_to_orig(original_corpus, stemmer):\n",
    "    counts = defaultdict(lambda: defaultdict(int))\n",
    "    surface_forms={}\n",
    "    for document in original_corpus:\n",
    "        for token in document:\n",
    "            stemmed = stemmer.stem(token)\n",
    "            counts[stemmed][token]+=1\n",
    "    \n",
    "    for stemmed, originals in counts.items():\n",
    "        surface_forms[stemmed] = max(originals, key=lambda i: originals[i])\n",
    "    return surface_forms \n",
    "\n",
    "#remove words with len(word)<n\n",
    "#remove all digits\n",
    "#stem words, and create mapping of stemed words back to most representative natural word\n",
    "def stem_dict_map(word_tokenized_sent_list,map_stem_dict, stemmer,min_word_len = 2):\n",
    "    out = []\n",
    "    for sent in word_tokenized_sent_list:\n",
    "        words = []\n",
    "        for word in sent:\n",
    "            if len(word)>=min_word_len and not any(c.isdigit() for c in word):\n",
    "                words.append(map_stem_dict[stemmer.stem(word)])\n",
    "        out.append(words)\n",
    "    return out \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=1000, step=1)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(\"C:\\\\Users\\\\zjc10\\\\Desktop\\\\Projects\\\\data\\\\news\\\\webhose_news\\\\webhose_df.pickle\").head(1000).reset_index(drop=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document level text preprocessing \n",
    "#### to do \n",
    "    - remove special chars \n",
    "    - remove occurances of blank lists (ex. Website: Twitter: Facebook: Pinterest: Goodreads:)\n",
    "    - if sentence contains click here, flag it (dont remove, until we see what all is entailed with click here)\n",
    "    - remove abreviations (must build list to replace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add title as first sentence \n",
    "df['new_text'] = df.apply(lambda row: row['title']+' .'+row['text'],axis=1)\n",
    "\n",
    "#replace odd symbol used for apostrophe \n",
    "df['new_text'] = df['text'].apply(lambda x: x.replace('тАЩ',\"'\"))\n",
    "\n",
    "#remove all non printable text (non ansci-ii)\n",
    "df['new_text'] = df['new_text'].apply(lambda x: re.sub(r'[^\\x00-\\x7f]',r'', x))\n",
    "\n",
    "#replace email and urls \n",
    "df['new_text'] = df['new_text'].apply(lambda x: remove_URL(x))\n",
    "df['new_text'] = df['new_text'].apply(lambda x: remove_emails(x))\n",
    "\n",
    "#replace double spaces, replace newline and new tab and \\r references, and strip outputand normalize to unicode \n",
    "df['new_text'] = df['new_text'].apply(lambda x: clean_unicode_text(x))   \n",
    "\n",
    "#remove contractions \n",
    "df['new_text'] = df['new_text'].apply(lambda x: replace_contractions(x))\n",
    "\n",
    "#extract parenthesesis text \n",
    "df['parentheses_txt'] = df['new_text'].apply(lambda x:extract_parentheses_text(x))\n",
    "df['new_text'] = df.apply(lambda row: remove_parentheses_txt(row['new_text'],row['parentheses_txt']),axis=1)\n",
    "\n",
    "#replace apostrophres and multiple periods\n",
    "df['new_text'] = df['new_text'].apply(lambda x: remove_multiple_periods(x.replace(\"'\",\"\")))\n",
    "\n",
    "#correct ALL UPPER case \n",
    "df['new_text'] = capcase_abrevs(df['new_text'],stop_list)\n",
    "\n",
    "#get ner information \n",
    "# df['ner_tags']= df['new_text'].apply(\n",
    "#     lambda txt: [ner_sent(nlp,sent)for sent in tokenize_sent(txt)])\n",
    "\n",
    "# df['pos_tags']= df['new_text'].apply(\n",
    "#     lambda txt: [pos_sent(nlp,sent)for sent in tokenize_sent(txt)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Level Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove repeat words and subset sent length\n",
    "df['txt2model'] = df['new_text'].apply(lambda txt:[   \n",
    "        ''.join(remove_repeatwords(sent))\n",
    "        for sent in tokenize_sent(txt) if len(sent)>20 ]\n",
    "                                      )\n",
    "#remove punct\n",
    "df['txt2model']  = df['txt2model'].apply(\n",
    "   lambda _list: [re.sub(f'[{string.punctuation}]',' ',x) for x in _list])\n",
    "\n",
    "#remove stopwords and clean string up \n",
    "df['txt2model']  = df['txt2model'].apply(\n",
    "   lambda _list: [clean_text_str(' '.join(replace_stopwords(tokenize_words(x),stop_words = stopwords))\n",
    "                        )\n",
    "                  for x in _list\n",
    "                 ])\n",
    "\n",
    "#lower it \n",
    "df['txt2model']  = df['txt2model'].apply(\n",
    "    lambda _list: [tokenize_words(x.lower()) for x in _list\n",
    "                  ])\n",
    "\n",
    "#stem and map stems to dict , filter common words , generate phrases , create nmf string\n",
    "map_stems = map_stems_to_orig([item for sublist in list(df['txt2model']) for item in sublist],stemmer)\n",
    "\n",
    "#if there is any processing to do at the word level (incorporate it here)\n",
    "df['txt2model_stem']  = df['txt2model'].apply(lambda sent_word_toks: stem_dict_map(sent_word_toks,map_stems,stemmer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "#incorp phrases into sentences \n",
    "df['txt2model_phrased'],phrase_dic =  incorp_phrases(df['txt2model_stem'], threshold=.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 3),\n",
       " (4, 1),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 8),\n",
       " (9, 1),\n",
       " (10, 1),\n",
       " (11, 1),\n",
       " (12, 2),\n",
       " (13, 1),\n",
       " (14, 1),\n",
       " (15, 2),\n",
       " (16, 1),\n",
       " (17, 1),\n",
       " (18, 1),\n",
       " (19, 1),\n",
       " (20, 1),\n",
       " (21, 1),\n",
       " (22, 1),\n",
       " (23, 1),\n",
       " (24, 1),\n",
       " (25, 1),\n",
       " (26, 1),\n",
       " (27, 1),\n",
       " (28, 1),\n",
       " (29, 2),\n",
       " (30, 1),\n",
       " (31, 1),\n",
       " (32, 1),\n",
       " (33, 1),\n",
       " (34, 1),\n",
       " (35, 1),\n",
       " (36, 1),\n",
       " (37, 1),\n",
       " (38, 1),\n",
       " (39, 1),\n",
       " (40, 1),\n",
       " (41, 1),\n",
       " (42, 1),\n",
       " (43, 1),\n",
       " (44, 1),\n",
       " (45, 1),\n",
       " (46, 1),\n",
       " (47, 1),\n",
       " (48, 1),\n",
       " (49, 1),\n",
       " (50, 1),\n",
       " (51, 1),\n",
       " (52, 1),\n",
       " (53, 1),\n",
       " (54, 1),\n",
       " (55, 1),\n",
       " (56, 1),\n",
       " (57, 1),\n",
       " (58, 1),\n",
       " (59, 1),\n",
       " (60, 1),\n",
       " (61, 1),\n",
       " (62, 1),\n",
       " (63, 1),\n",
       " (64, 1),\n",
       " (65, 3),\n",
       " (66, 1),\n",
       " (67, 1),\n",
       " (68, 1),\n",
       " (69, 1),\n",
       " (70, 1),\n",
       " (71, 1)]"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GENERATE NMF INPUT DICT / BOW\n",
    "#iterate over docs and populate gensim dict to be converted to bow  \n",
    "gensim_dict = None\n",
    "docs = [[item for sublist in doc for item in sublist] for doc in df['txt2model_phrased']]\n",
    "gensim_dict = create_gensim_dict(docs)\n",
    "\n",
    "#filter out tokens taht appear in less than no_below docs and appear no more than in no_above of all docs , keeping the top keep_nmost frequent tokens \n",
    "gensim_dict.filter_extremes(no_below=2, no_above=0.1, keep_n=None)\n",
    "bow_corpus = [gensim_dict.doc2bow(doc,allow_update=False) for doc in docs]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'map' and 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-327-93eea77f2eed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m#difference in original string vs scrubbed string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOriginal\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mNoStopWrds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No Stop Words Found in:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Strings\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Stop Words removed from:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Strings\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'map' and 'map'"
     ]
    }
   ],
   "source": [
    "#explicitly filtering out common words from each bow based on tfidf weights\n",
    "import numpy as np\n",
    "\n",
    "CORPUS = bow_corpus\n",
    "low_value = .05\n",
    "tfidf = gensim.models.TfidfModel(CORPUS, id2word = gensim_dict)\n",
    "\n",
    "\n",
    "for i in range(0,len(CORPUS)):\n",
    "    bow = bow_corpus[i]\n",
    "    \n",
    "    low_value_words = [] #reinitialize to be safe. You can skip this.\n",
    "    low_value_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "    new_bow = [b for b in bow if b[0] not in low_value_words]\n",
    "    \n",
    "    #reassign \n",
    "    CORPUS[i] = new_bow \n",
    "    \n",
    "#length of each new corpus with stop words removed \n",
    "Original = np.array(map(len,bow_corpus)) \n",
    "NoStopWrds = np.array(map(len,CORPUS))\n",
    "\n",
    "#difference in original string vs scrubbed string\n",
    "diff = Original-NoStopWrds\n",
    "print(\"No Stop Words Found in:\", len(diff[diff==0]),\"Strings\")\n",
    "print(\"Stop Words removed from:\",len(diff[diff>0]),\"Strings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gensim dict operators \n",
    "def create_gensim_dict(word_tokenized_sents_list):\n",
    "    return gensim.corpora.Dictionary(word_tokenized_sents_list)\n",
    "    \n",
    "def update_gensim_dict(dict2Update, word_tokenized_sents_list):\n",
    "    return dict2Update.add_documents(word_tokenized_sents_list)\n",
    "\n",
    "\n",
    "#convert each doc to ids_ after all words from all docs have been accounted for in gensim dict \n",
    "\n",
    "#gensim_dict: gensim dict capturing all words / freqs across all documents \n",
    "def dict2doc2bow_corpus(gensim_dict,word_tokenized_doc_list):\n",
    "    corpus_out = [gensim_dict.doc2bow(doc,allow_update=True) for doc in word_tokenized_doc_list]\n",
    "    print([[(gensim_dict[id], count) for id,count in line] for line in word_tokenized_doc_list])\n",
    "    return corpus_out\n",
    "\n",
    "#get indiv doc word freqs from dict and bowcorpp \n",
    "#ex. first_doc_word_freqs = get_doc_word_freq(gensim_dict, bow_corpus[0])\n",
    "def get_doc_word_freq(gensim_dict, bow_corpus):\n",
    "    return  [(gensim_dict[id], count) for id, count in bow_corpus]\n",
    "\n",
    "\n",
    "\n",
    "#save gensim dict and corpus to disk and load \n",
    "#note: filepath must have .dict extension \n",
    "def save_gensim_dict(gensim_dict, filepath):\n",
    "    gensim_dict.save(filepath)\n",
    "    print('gensim dict saved at {}'.format(filepath))\n",
    "    \n",
    "#save corpus to dict \n",
    "#note filepath must end in .mm\n",
    "def save_gensim_corp(gesim_bow_corp, filepath):\n",
    "    gensim_corp.MmCorpus.serialize(filepath,gesim_bow_corp)\n",
    "          \n",
    "def load_gensim_dict(filepath):\n",
    "    return corpora.Dictionary.load(filepath)\n",
    "\n",
    "def load_gensim_corpus(filepath):\n",
    "    return corpora.MmCorpus(filepath)\n",
    "    \n",
    "    \n",
    "#create dictionary to filter out common terms \n",
    "#dictionary = gensim.corpora.Dictionary(df['txt2model_phrased'].astype(str))\n",
    "#df['txt2model_phrased'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_docs(df=None, textcol_name = None):\n",
    "    if not df:\n",
    "        raise exception('error must provide dataframe')\n",
    "        \n",
    "    if not textcol_name: \n",
    "        #add title as first sentence \n",
    "        df['new_text'] = df.apply(lambda row: row['title']+' .'+row['text'],axis=1)\n",
    "    else:\n",
    "        df['new_text'] = df.apply(lambda row: row['textcol_name'],axis=1)\n",
    "\n",
    "    #replace odd symbol used for apostrophe \n",
    "    df['new_text'] = df['text'].apply(lambda x: x.replace('тАЩ',\"'\"))\n",
    "\n",
    "    #remove all non printable text (non ansci-ii)\n",
    "    df['new_text'] = df['new_text'].apply(lambda x: re.sub(r'[^\\x00-\\x7f]',r'', x))\n",
    "\n",
    "    #replace email and urls \n",
    "    df['new_text'] = df['new_text'].apply(lambda x: remove_URL(x))\n",
    "    df['new_text'] = df['new_text'].apply(lambda x: remove_emails(x))\n",
    "\n",
    "    #replace double spaces, replace newline and new tab and \\r references, and strip outputand normalize to unicode \n",
    "    df['new_text'] = df['new_text'].apply(lambda x: clean_unicode_text(x))   \n",
    "\n",
    "    #remove contractions \n",
    "    df['new_text'] = df['new_text'].apply(lambda x: replace_contractions(x))\n",
    "\n",
    "    #extract parenthesesis text \n",
    "    df['parentheses_txt'] = df['new_text'].apply(lambda x:extract_parentheses_text(x))\n",
    "    df['new_text'] = df.apply(lambda row: remove_parentheses_txt(row['new_text'],row['parentheses_txt']),axis=1)\n",
    "\n",
    "    #replace apostrophres and multiple periods\n",
    "    df['new_text'] = df['new_text'].apply(lambda x: remove_multiple_periods(x.replace(\"'\",\"\")))\n",
    "\n",
    "    #correct ALL UPPER case \n",
    "    df['new_text'] = capcase_abrevs(df['new_text'],stop_list)\n",
    "\n",
    "    #get ner information \n",
    "    # df['ner_tags']= df['new_text'].apply(\n",
    "    #     lambda txt: [ner_sent(nlp,sent)for sent in tokenize_sent(txt)])\n",
    "\n",
    "    # df['pos_tags']= df['new_text'].apply(\n",
    "    #     lambda txt: [pos_sent(nlp,sent)for sent in tokenize_sent(txt)])\n",
    "\n",
    "    return df \n",
    "\n",
    "def process_sentences(df)\n",
    "\n",
    "    #remove repeat words and subset sent length\n",
    "    df['txt2model'] = df['new_text'].apply(lambda txt:[   \n",
    "            ''.join(remove_repeatwords(sent))\n",
    "            for sent in tokenize_sent(txt) if len(sent)>20 ]\n",
    "                                          )\n",
    "    #remove punct\n",
    "    df['txt2model']  = df['txt2model'].apply(\n",
    "       lambda _list: [re.sub(f'[{string.punctuation}]',' ',x) for x in _list])\n",
    "\n",
    "    #remove stopwords and clean string up \n",
    "    df['txt2model']  = df['txt2model'].apply(\n",
    "       lambda _list: [clean_text_str(' '.join(replace_stopwords(tokenize_words(x),stop_words = stopwords))\n",
    "                            )\n",
    "                      for x in _list\n",
    "                     ])\n",
    "\n",
    "    #lower it \n",
    "    df['txt2model']  = df['txt2model'].apply(\n",
    "        lambda _list: [tokenize_words(x.lower()) for x in _list\n",
    "                      ])\n",
    "\n",
    "    #stem and map stems to dict , filter common words , generate phrases , create nmf string\n",
    "    map_stems = map_stems_to_orig([item for sublist in list(df['txt2model']) for item in sublist],stemmer)\n",
    "\n",
    "    #if there is any processing to do at the word level (incorporate it here)\n",
    "    df['txt2model_stem']  = df['txt2model'].apply(lambda sent_word_toks: stem_dict_map(sent_word_toks,map_stems,stemmer))\n",
    "    \n",
    "    return df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_dict = gensim.corpora.Dictionary()\n",
    "len(gensim_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "a =gensim.corpora.Dictionary([item for sublist in df['txt2model_phrased'].head(1) for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'changer')\n",
      "(1, 'game')\n",
      "(2, 'jami_davenport')\n",
      "(3, 'live')\n",
      "(4, 'anything')\n",
      "(5, 'discovery')\n",
      "(6, 'else')\n",
      "(7, 'past')\n",
      "(8, 'pulled')\n",
      "(9, 'read')\n",
      "(10, 'really')\n",
      "(11, 'refreshing')\n",
      "(12, 'story')\n",
      "(13, 'unlike')\n",
      "(14, 'heartstrings')\n",
      "(15, 'high')\n",
      "(16, 'leave')\n",
      "(17, 'one')\n",
      "(18, 'recommend')\n",
      "(19, 'smile')\n",
      "(20, 'sure')\n",
      "(21, 'tug')\n",
      "(22, 'buy')\n",
      "(23, 'red')\n",
      "(24, 'review')\n",
      "(25, 'romance')\n",
      "(26, 'amazon')\n",
      "(27, 'battle')\n",
      "(28, 'blurb')\n",
      "(29, 'champion')\n",
      "(30, 'collide')\n",
      "(31, 'end')\n",
      "(32, 'find')\n",
      "(33, 'football')\n",
      "(34, 'horse')\n",
      "(35, 'horsewoman_stubborn')\n",
      "(36, 'hunter')\n",
      "(37, 'ibooks')\n",
      "(38, 'kobo')\n",
      "(39, 'mccoy')\n",
      "(40, 'neophyte')\n",
      "(41, 'nook')\n",
      "(42, 'owners')\n",
      "(43, 'race')\n",
      "(44, 'seattle')\n",
      "(45, 'sexy')\n",
      "(46, 'star')\n",
      "(47, 'steelheads')\n",
      "(48, 'thoroughbred')\n",
      "(49, 'tight')\n",
      "(50, 'willing')\n",
      "(51, 'beat')\n",
      "(52, 'carrigans')\n",
      "(53, 'derby')\n",
      "(54, 'died')\n",
      "(55, 'enough')\n",
      "(56, 'enter')\n",
      "(57, 'farm')\n",
      "(58, 'feat')\n",
      "(59, 'fulfillment')\n",
      "(60, 'heart')\n",
      "(61, 'kate')\n",
      "(62, 'mother')\n",
      "(63, 'order')\n",
      "(64, 'save')\n",
      "(65, 'vanderhof')\n",
      "(66, 'wish')\n",
      "(67, 'believe')\n",
      "(68, 'boyfriend')\n",
      "(69, 'burn')\n",
      "(70, 'last')\n",
      "(71, 'lilli')\n",
      "(72, 'men')\n",
      "(73, 'sister')\n",
      "(74, 'blind')\n",
      "(75, 'cheat')\n",
      "(76, 'deals')\n",
      "(77, 'disability')\n",
      "(78, 'tools')\n",
      "(79, 'using')\n",
      "(80, 'betrayal')\n",
      "(81, 'home')\n",
      "(82, 'lick')\n",
      "(83, 'reeling')\n",
      "(84, 'return')\n",
      "(85, 'wounds')\n",
      "(86, 'attractive')\n",
      "(87, 'brother')\n",
      "(88, 'cameron')\n",
      "(89, 'factors')\n",
      "(90, 'feel')\n",
      "(91, 'instant')\n",
      "(92, 'playboy')\n",
      "(93, 'swift')\n",
      "(94, 'teammate')\n",
      "(95, 'life')\n",
      "(96, 'excerpt')\n",
      "(97, 'man')\n",
      "(98, 'never')\n",
      "(99, 'regarding')\n",
      "(100, 'seen')\n",
      "(101, 'staring')\n",
      "(102, 'turned')\n",
      "(103, 'woman')\n",
      "(104, 'happened')\n",
      "(105, 'nothing')\n",
      "(106, 'per')\n",
      "(107, 'sign')\n",
      "(108, 'usually')\n",
      "(109, 'wait')\n",
      "(110, 'mates')\n",
      "(111, 'recognition')\n",
      "(112, 'soul')\n",
      "(113, 'thrilled')\n",
      "(114, 'quickening')\n",
      "(115, 'finally')\n",
      "(116, 'sense')\n",
      "(117, 'expected')\n",
      "(118, 'mind')\n",
      "(119, 'open')\n",
      "(120, 'pride')\n",
      "(121, 'approach')\n",
      "(122, 'blue')\n",
      "(123, 'eyes')\n",
      "(124, 'glinted')\n",
      "(125, 'linebacker')\n",
      "(126, 'purpose')\n",
      "(127, 'pursuing')\n",
      "(128, 'quarterback')\n",
      "(129, 'single')\n",
      "(130, 'braced')\n",
      "(131, 'dismissed')\n",
      "(132, 'doubt')\n",
      "(133, 'kind')\n",
      "(134, 'political')\n",
      "(135, 'ready')\n",
      "(136, 'though')\n",
      "(137, 'fan')\n",
      "(138, 'perhaps')\n",
      "(139, 'always')\n",
      "(140, 'american')\n",
      "(141, 'coastal')\n",
      "(142, 'interview')\n",
      "(143, 'league')\n",
      "(144, 'members')\n",
      "(145, 'model')\n",
      "(146, 'native')\n",
      "(147, 'photo')\n",
      "(148, 'players')\n",
      "(149, 'role')\n",
      "(150, 'salish')\n",
      "(151, 'sessions')\n",
      "(152, 'sought')\n",
      "(153, 'tribal')\n",
      "(154, 'tribe')\n",
      "(155, 'youth')\n",
      "(156, 'average')\n",
      "(157, 'best')\n",
      "(158, 'campaign')\n",
      "(159, 'culture')\n",
      "(160, 'every')\n",
      "(161, 'meet')\n",
      "(162, 'part')\n",
      "(163, 'personal')\n",
      "(164, 'request')\n",
      "(165, 'understand')\n",
      "(166, 'hand')\n",
      "(167, 'held')\n",
      "(168, 'hello')\n",
      "(169, 'back')\n",
      "(170, 'exact')\n",
      "(171, 'assessment')\n",
      "(172, 'instead')\n",
      "(173, 'look')\n",
      "(174, 'strength')\n",
      "(175, 'weak')\n",
      "(176, 'firm')\n",
      "(177, 'grip')\n",
      "(178, 'shook')\n",
      "(179, 'slightly')\n",
      "(180, 'surprise')\n",
      "(181, 'formidable')\n",
      "(182, 'honed')\n",
      "(183, 'instinct')\n",
      "(184, 'opponent')\n",
      "(185, 'warned')\n",
      "(186, 'well')\n",
      "(187, 'beauty')\n",
      "(188, 'classic')\n",
      "(189, 'crap')\n",
      "(190, 'little')\n",
      "(191, 'society')\n",
      "(192, 'standard')\n",
      "(193, 'advantage')\n",
      "(194, 'around')\n",
      "(195, 'assets')\n",
      "(196, 'brown')\n",
      "(197, 'burnished')\n",
      "(198, 'curling')\n",
      "(199, 'hair')\n",
      "(200, 'makeup')\n",
      "(201, 'perfect')\n",
      "(202, 'reddish')\n",
      "(203, 'shoulder')\n",
      "(204, 'subtle')\n",
      "(205, 'tall')\n",
      "(206, 'took')\n",
      "(207, 'yet')\n",
      "(208, 'bury')\n",
      "(209, 'fingers')\n",
      "(210, 'stifling')\n",
      "(211, 'urge')\n",
      "(212, 'destined')\n",
      "(213, 'interesting')\n",
      "(214, 'like')\n",
      "(215, 'night')\n",
      "(216, 'stand')\n",
      "(217, 'whod')\n",
      "(218, 'actually')\n",
      "(219, 'ahead')\n",
      "(220, 'cost')\n",
      "(221, 'type')\n",
      "(222, 'uptight')\n",
      "(223, 'everything')\n",
      "(224, 'money')\n",
      "(225, 'reeked')\n",
      "(226, 'care')\n",
      "(227, 'libido')\n",
      "(228, 'much')\n",
      "(229, 'seems')\n",
      "(230, 'barnes')\n",
      "(231, 'known')\n",
      "(232, 'point')\n",
      "(233, 'seastrong')\n",
      "(234, 'sid')\n",
      "(235, 'puffed')\n",
      "(236, 'getting')\n",
      "(237, 'said')\n",
      "(238, 'consider')\n",
      "(239, 'impatient')\n",
      "(240, 'indicate')\n",
      "(241, 'moron')\n",
      "(242, 'sort')\n",
      "(243, 'tone')\n",
      "(244, 'understood')\n",
      "(245, 'clothes')\n",
      "(246, 'groom')\n",
      "(247, 'quality')\n",
      "(248, 'trainer')\n",
      "(249, 'wore')\n",
      "(250, 'head')\n",
      "(251, 'laugh')\n",
      "(252, 'threw')\n",
      "(253, 'fifteen')\n",
      "(254, 'investors')\n",
      "(255, 'selling')\n",
      "(256, 'wanted')\n",
      "(257, 'entire')\n",
      "(258, 'true')\n",
      "(259, 'athletic')\n",
      "(260, 'comprises')\n",
      "(261, 'manage')\n",
      "(262, 'team')\n",
      "(263, 'circle')\n",
      "(264, 'kentucky')\n",
      "(265, 'none')\n",
      "(266, 'short')\n",
      "(267, 'show')\n",
      "(268, 'winner')\n",
      "(269, 'accomplish')\n",
      "(270, 'long')\n",
      "(271, 'shot')\n",
      "(272, 'everyone')\n",
      "(273, 'price')\n",
      "(274, 'second')\n",
      "(275, 'thought')\n",
      "(276, 'family')\n",
      "(277, 'heartbroken')\n",
      "(278, 'sold')\n",
      "(279, 'connect')\n",
      "(280, 'cross')\n",
      "(281, 'face')\n",
      "(282, 'skepticism')\n",
      "(283, 'maybe')\n",
      "(284, 'talk')\n",
      "(285, 'animal')\n",
      "(286, 'attached')\n",
      "(287, 'appear')\n",
      "(288, 'hear')\n",
      "(289, 'women')\n",
      "(290, 'words')\n",
      "(291, 'intent')\n",
      "(292, 'narrow')\n",
      "(293, 'study')\n",
      "(294, 'available')\n",
      "(295, 'ludicrous')\n",
      "(296, 'snorted')\n",
      "(297, 'statement')\n",
      "(298, 'reason')\n",
      "(299, 'demeanor')\n",
      "(300, 'hell')\n",
      "(301, 'icy')\n",
      "(302, 'melted')\n",
      "(303, 'panties')\n",
      "(304, 'assured')\n",
      "(305, 'conceited')\n",
      "(306, 'confidence')\n",
      "(307, 'self')\n",
      "(308, 'prime')\n",
      "(309, 'professional')\n",
      "(310, 'flocked')\n",
      "(311, 'autograph')\n",
      "(312, 'closer')\n",
      "(313, 'date')\n",
      "(314, 'waste')\n",
      "(315, 'wrong')\n",
      "(316, 'sales')\n",
      "(317, 'author')\n",
      "(318, 'bestselling')\n",
      "(319, 'contemporary')\n",
      "(320, 'endeavors')\n",
      "(321, 'including')\n",
      "(322, 'indie')\n",
      "(323, 'island')\n",
      "(324, 'madrona')\n",
      "(325, 'new')\n",
      "(326, 'series')\n",
      "(327, 'sports')\n",
      "(328, 'today')\n",
      "(329, 'two')\n",
      "(330, 'usa')\n",
      "(331, 'writing')\n",
      "(332, 'consistently')\n",
      "(333, 'fiction')\n",
      "(334, 'fifty')\n",
      "(335, 'genre')\n",
      "(336, 'hit')\n",
      "(337, 'hundreds')\n",
      "(338, 'jami')\n",
      "(339, 'list')\n",
      "(340, 'multiple')\n",
      "(341, 'rank')\n",
      "(342, 'release')\n",
      "(343, 'times')\n",
      "(344, 'top')\n",
      "(345, 'based')\n",
      "(346, 'bestowed')\n",
      "(347, 'complete')\n",
      "(348, 'engaging')\n",
      "(349, 'honor')\n",
      "(350, 'number')\n",
      "(351, 'pages')\n",
      "(352, 'rate')\n",
      "(353, 'readers')\n",
      "(354, 'seven')\n",
      "(355, 'ten')\n",
      "(356, 'years')\n",
      "(357, 'ball')\n",
      "(358, 'beret')\n",
      "(359, 'cat')\n",
      "(360, 'disguised')\n",
      "(361, 'fetish')\n",
      "(362, 'green')\n",
      "(363, 'hanoverian_mare')\n",
      "(364, 'husband')\n",
      "(365, 'near')\n",
      "(366, 'newfoundland')\n",
      "(367, 'opinion')\n",
      "(368, 'orange_tabby')\n",
      "(369, 'plumber')\n",
      "(370, 'prince')\n",
      "(371, 'puget')\n",
      "(372, 'small')\n",
      "(373, 'sound')\n",
      "(374, 'tennis')\n",
      "(375, 'business')\n",
      "(376, 'day')\n",
      "(377, 'former')\n",
      "(378, 'job')\n",
      "(379, 'school')\n",
      "(380, 'teacher')\n",
      "(381, 'work')\n",
      "(382, 'comes')\n",
      "(383, 'hockey')\n",
      "(384, 'lifetime_seahawks')\n",
      "(385, 'marine')\n",
      "(386, 'avid_boater')\n",
      "(387, 'book')\n",
      "(388, 'common')\n",
      "(389, 'countless')\n",
      "(390, 'hours')\n",
      "(391, 'san_juan')\n",
      "(392, 'set')\n",
      "(393, 'spent')\n",
      "(394, 'earth')\n",
      "(395, 'place')\n",
      "(396, 'contest')\n",
      "(397, 'facebook')\n",
      "(398, 'free')\n",
      "(399, 'goodreads')\n",
      "(400, 'newsletter')\n",
      "(401, 'notified')\n",
      "(402, 'novel')\n",
      "(403, 'pinterest')\n",
      "(404, 'receive')\n",
      "(405, 'special')\n",
      "(406, 'subscribe')\n",
      "(407, 'twitter')\n",
      "(408, 'website')\n"
     ]
    }
   ],
   "source": [
    "for val in gensim_dict.items():\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##FILTER OUT LOW INFORMATION WORDS \n",
    "##NEED TO GET THIS WORKING\n",
    "def filterCommonWords(BOWCoprus,dictionary, corpus, low_value_thresh = .02):\n",
    "\tfrom gensim.models import TfidfModel\n",
    "\tfrom gensim import models\n",
    "\t#filter out common words \n",
    "\t#save copy of original corpus \n",
    "\tCORPUS = list(BOWCoprus)\n",
    "\n",
    "\t#create td-idf model object using dictonary\n",
    "\ttfidf = models.TfidfModel(CORPUS, id2word = dictionary)\n",
    "\n",
    "\t#filter low value words\n",
    "\tlow_value = low_value_thresh\n",
    "\n",
    "\tfor i in range(0, len(CORPUS)):\n",
    "\t    bow = corpus[i]\n",
    "\t    low_value_words = [] #reinitialize to be safe. You can skip this.\n",
    "\t    low_value_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "\t    new_bow = [b for b in bow if b[0] not in low_value_words]\n",
    "\n",
    "\t    #reassign        \n",
    "\t    CORPUS[i] = new_bow\n",
    "\t    \n",
    "\t#length of each new corpus with stop words removed \n",
    "\tOriginal = np.array(map(len,corpus)) \n",
    "\tNoStopWrds = np.array(map(len,CORPUS))\n",
    "\n",
    "\t#difference in original string vs scrubbed string\n",
    "\tdiff = Original-NoStopWrds\n",
    "\tprint(\"No Stop Words Found in:\", len(diff[diff==0]),\"Strings\")\n",
    "\tprint(\"Stop Words removed from:\",len(diff[diff>0]),\"Strings\")\n",
    "\n",
    "\treturn CORPUS   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [[item for sublist in doc for item in sublist] for doc in df['txt2model_phrased']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#preprocess docs with gensim \n",
    "stemlem_docs = df['txt2model_phrased'].astype(str).map(preprocess)\n",
    "#stemlem_docs=preprocessed_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['new_text'].tail(30)\n",
    "#sample = [x for x in df['new_text']]\n",
    "\n",
    "#findit = re.compile(r\"(?<=\\.)\\s+(?=com)\")\n",
    "#findit.findall(s)\n",
    "\n",
    "m = re.search(r\"(?<=\\.)\\s+(?=com)\",s,re.MULTILINE)\n",
    "m = re.search(r'\\([^)]*\\)', s, re.MULTILINE)\n",
    "\n",
    "m.span()\n",
    "s[m.span()[0]-10:m.span()[1]+10]\n",
    "\n",
    "\n",
    "if matches:\n",
    "#   print(matches.groups())\n",
    "   print(matches)\n",
    "    \n",
    "#    return  re.sub(r\"[^\\s]*\\.(com|org|net|gov)\\S*\", r'', re.sub(r\"www//.S+\", \"\",re.sub(r\"http\\S+\", \"\", fixspaces)))\n",
    "#)\n",
    "#s\n",
    "#import dateparser\n",
    "#from dateutil.parser import parse\n",
    " \n",
    "#parse(s[:300], fuzzy_with_tokens=True)\n",
    "#type(s)\n",
    "#import phonenumbers \n",
    "\n",
    "# text = \"Call me at 5107488230 if it's before 9:30, or on 703-4800500 after 10am.\"\n",
    "# for match in phonenumbers.PhoneNumberMatcher(str(sample[136])):\n",
    "#     print(match)\n",
    "df['new_text'].tail(12)\n",
    "\n",
    "#Is this what you desire?\n",
    "\n",
    "#import re\n",
    "\n",
    "\n",
    "#re.sub(r\"\\.+(?! )\", \". \", 'my name is zack..i dont lik you.ur not cool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [[world, south, america, brazil, soundtrack], [okay, maybe, exact, brazillian, music, awesome, live, show, legendary], [brazil, cover, almost, half, south, america, amazon, rainforest, world, largest, jungle], [rapid, getting, cut], [country, basic, one, giant, botanical, garden, bangin, city, edge], [brazil, colon, royal, court, portugal, fleeing, napoleans, troops], [stay, long, brazil, independent, biggest, city, sao, paulo, financial, hub, south, america], [brazil, bric, brazil, russia, india, china], [four, label, world, fastest, development, large, economy], [brazil, known, three, things, amazing, beauty, women, carnival, pele, king, football, athletic, century, football, ambassador, world, declared, national, treasure], [brazillian, tell, foreign, miles, away, way, hips, move], [samba, built, soul, brazil, carnival, burst, twenty, four, hours, undying, explosive, sound, every, street], [picture, mask, dive, taipus, reefs, making, hot, salty, tears, calendar, set, january]]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "1    [[post, dr, pete, seo, worth, sustainably, harvested, pink, himalayan, salt, knows, offer, variety, advanced, search, operator, special, commands, beyond, regular, text, search], [learn, search, operator, bit, like, learn, chess, though], [memorize, piece, move, path, toward, mastery], [pointy, hat, guy, chess, move, diagonally, means, kasparov, deep, blue], [instead, list, operator, tell, like, something, different], [post, journey, part, split, five, functionality, story, content, research, title, research, plagiarism, check, competitive, research, technical, seo, audit, skip, around, suggest, following, story, beginning], [understand, operator, real, world, situation, mix, match, using, operator], [content, research, crafting, original, content, requires, wading, sea, content, already, create, remaining, complete, map, sea], [advanced, search, operator, invaluable, research, tools, content, market], [walk, sample, content, journey], [find, content, tesla, blog, post, writing, inventor, nikola, tesla], [hop, search, tesla, find, results, like, decided, tesla, motors, dominant, intent, phrase, help, much, current, project], [narrow, search, nikola, tesla, course, add, keywords, narrow, search], [track, anyone, ever, search, understand, important, point, often, overlook], [whenever, string, together, one, words, search, connect, logical], [true, keywords, operator], [combine, operator, assume, meant, meet, conditions], [mind, special, characters, tesla, ac, dc, specific, find, pages, phrase, ac, dc, search, notice, highlights, words, return, anything, match, ac, dc, separate], [case, treated, forward, slash, space, probably, intended], [force, exact, match, quote, tesla, ac, dc, put, quotation, mark, around, phrase, force, exact, match, search], [requires, match, specific, full, phrase, terms, order, specify, closer, probably, expected], [notice, highlights, second, results, seems, match, ac, dc], [closer, previous, attempt, still, taking, liberty, forward, slash], [sure, sanity, check, results, non, alphanumeric, characters, search], [force, logical, tesla, edison, specific, logical, keywords, operator, operator], [must, caps, alternatively, pipe, symbol, note, case, still, going, give, priority, results, contain, terms], [specify, logical, using, two, terms, co, occur, rare], [group, terms, parentheses, alternatively, current, operator, including, using, complex, search], [using, parentheses, group, tesla, edison, adding, alternatively, current, conditions, requires, three, terms, might, unnecessarily, restrictive], [using, ands, ors, search, give, bit, flexibility], [since, probably, memorize, precedence, search, operator, high, recommend, using, parentheses, whenever, doubt], [exclude, specific, terms, tesla, motors, maybe, using, tesla, beyond, tesla, motors], [operator, tell, exclude, results, motors, browsing, results, quickly, tesla, band, united, measures], [additional, tesla, company, making, product, car], [keywords, exclusions, called, negative, keywords], [exclude, multiple, terms, tesla, motors, car, battery, like, positive, keywords, chain, together, negative, keywords, keep, mind, minus, sign, pair, single, keywords, operator], [exclude, exact, match, phrase, tesla, motors, rock, roll, exclude, full, phrase, using, sign, following, phrase, quote, combine, individual, negative, keywords, negative, exact, match, phrase, needed], [match, broad, wildcard, tesla, motors, rock, roll, specific, wanted, including, rock, roll, band, care, whether, spelled, rock, roll, rock, roll, rock, roll, etc], [asterisk, operator, wildcard, replace, single, words, wildcard, behave, predictably, within, exact, match, phrase, allow, find, near, match, pin, search, single, phrase], [operator, operator, words, level], [single, characters, wildcard, operator], [find, terms, near, tesla, around, edison, nifty, one], [maybe, find, results, tesla, edison, appear, document, fairly, close], [around, operator, tell, return, results, two, words, within, words, phrase, like, tesla, vs, thomas, edison, show, match, article, two, men, mention, separate, paragraphs], [find, near, exact, match, phrase, nikola, tesla, around, thomas, alva, edison, reason, really, needed, reference, including, full, name], [combine, around, exact, match, phrase, around, work, entities, immediately, precedence, following, care, combine, operator, phrase, exact, match], [note, around, return, strange, results, return, two, words, appear, together, exact, match, phrase, instead], [find, content, specific, site, nikola, tesla, site, operator, advanced, commands, specify, specific, domain, search], [usually, technical, seo, audit, tools, help, refine, content, search], [remember, read, article, pbs, tesla, lost, url, typically, site, root, domain, match, broad, possible], [advanced, operator, like, site, combine, keywords], [find, content, specific, tlds, nikola, tesla, site, including, full, domain, site], [example, wanted, find, content, nikola, tesla, university, website], [search, domain, site, operator, work, partial, domain, name], [accept, full, domain, root, domain, tlds], [country, specific, tlds, co, uk, com], [find, content, multiple, tlds, nikola, tesla, keywords, combine, site, operator, logical, search, multiple, domain, often, easier, bit, less, confusing, individual, search, example, illustrate, combine, advanced, operator, complex, way], [deals, broad, match, discount, airfare, getting, better, match, synonyms, usually, sometimes, means, results, broader, might, expected, search, discount, airfare, return, keywords, like, cheapest, flights, cheap, flights, airfare, deals, variety, combine], [exact, match, block, synonyms, discount, airfare, another, situation, exact, match, help], [tell, full, phrase, block, return, kind, broad, match, including, synonyms, obviously, results, still, contain, synonyms, using, exact, match, ensure, least, one, instance, discount, airfare, results, back], [exact, match, single, words, discount, airfare, counter, intuitive, apply, exact, match, one, words], [case, put, exact, match, airfare, block, using, synonyms, words, free, match, synonyms, discount, every, results, force, including, airfare], [exact, match, single, words, exclude, variations, words], [exact, match, fails, orbi, vs, eero, vs, wifi, day, search, article, specific, compared, orbi, eero, wifi, network, hardware], [something, odd, happened, search, exact, match, phrase, obviously, search, results, first, results, contain, phrase, anywhere, body, text], [rare, occasion, match, phrase, secondary, relevant, factors, inbound, link, anchor, text], [search, body, text, intext, orbi, vs, eero, vs, wifi, rare, case, intext, operator], [force, find, text, body, document], [top, results, clear, exact, match, content, interesting, second, results, revealed, happened, last, search], [reddit, post, features, article, verge, alternatively, title, using, title, anchor, text], [reddit, apparently, enough, author, generate, match, via, anchor, text, alone], [find, set, keywords, text, allintext, orbi, eero, wifi, find, set, words, exact, match, phrase], [separate, intext, operator, words, allintext, tell, apply, intext, words, following, operator, results, target, keywords, body, text, combine, order], [care, mix, allintext, commands, end, unexpected, results], [allintext, operator, automatically, process, anything, following], [title, research, content, research, pin, title], [capture, click, course, unoriginal], [search, operator, combo, title, research], [check, specific, phrase, tesla, vs, edison, settled, using, tesla, vs, edison, title, quickly, check, content, exact, match, phrase, pin, exact, match, phrase, phrase, occur, anywhere, text], [look, document, title], [check, phrase, title, intitle, tesla, vs, edison, intitle, operator, specify, keywords, phrase, occur, document, title, aware, sometimes, rewrite, display, title, search, results, possible, results, back, phrase, match, title, rewritten], [check, multiple, keywords, title, intitle, tesla, intitle, vs, intitle, edison, check, multiple, keywords, title, restrictive, exact, match, string, together, multiple, intitle, operator, single, keywords, course, bit, clunky], [luckily, easier, way], [check, multiple, keywords, allintitle, tesla, vs, edison, like, allintext, allintitle, operator], [match, keywords, following, return, roughly, results, interesting, screenshot, exact], [care, combine, allintitle, operator, consumers, everything, following], [check, title, list, intitle, top, fact, tesla, maybe, heart, set, listicle, sure, death], [combine, intitle, operator, general, keywords, search, topic, results, pages, talk, tesla, top, fact, title], [find, list, exact, match, phrase, intitle, top, fact, nikola, tesla, oops, pulled, results, tesla, motors], [luckily, combine, intitle, exact, match, phrase, complex, operator, combo, much, closer, probably, mind, bad, news, top, things, like, overdone, realm, nikola, tesla], [check, top, list, intitle, top], [fact, nikola, tesla, range, operator, search, specific, range, number], [maybe, tired, top, short, list], [check, top, list, return, four, results, video], [least, track, original, wise], [master, search, operator, eventually, reach, mythical, end, internet], [check, title, post, intitle, search, operator, step, put, test, original, title, post], [expected, exact, match, post, step, post, mention, search, operator, title, using, variations, step, anywhere, results], [look, like, alright, original, standpoint], [course, way, mix, match, operator, find, similar, title], ...]\n",
       "2    [[getty, images, tuesday, youtube, unveiled, youtube, tv, month, competitors, cable, tv, delivered, package, streaming, channels, internet], [youtube, tv, company, jump, increase, crowd, market, service, look, lure, cord, cutter, cord, never, back, pay, tv], [dishes, sling, tv, ts, directv, sony, vue, already, fight, hulu, enter, fray, next, month], [amazon, rumors, work, tv, package], [one, give, youtube, early, edge, nail, technical, aspect, youtube, tv, something, current, players, struggle], [means, interface, though, plus, simply, making, sure, product, actually, work, especially, high, profile, event, viewers], [idea, live, streaming, tv, service, new, reference, point, people, tv, supposed, functionality, cable, satellite], [cable, occasionally, glitch, large, functionality, advertising], [watch, something], [said, early, streaming, tv, service], [since, launch, late, november, ts, directv, service, repeat, smacked, technical, issues, strange, error, message, huge, blackout], [sling, tv, vue, share, tech, snafus, well], [something, tend, taken, granted, actually, pretty, herculean, effort, years, video, serve, infrastructure, built, neal, mohan, youtube, head, product, told, bloomberg], [every, minutes, average, hours, content, uploaded, youtube, ready, serve], [enormous, amount, learn, went, apply, new, youtube, tv, experience, reliable, flawless], [youtube, ton, knowledge, area, already, delivered, online, video, live, online, video], [youtube, tv, actually, work, day, one, youtube, huge, opportunity, snag, market, share], [last, month, announcements, gotten, additional, pay, video, subscribe, quarter, end, december, entire, driven, directv], [end, directv, business, month], [pay, subscribe, month, impressions, means, demand, product], [far, directv, delivered, frustration, experience, early, adopted, taken, forum, twitter, rant, service], [open, youtube], [hole, problem, youtube, might, tech, content, deals, sign], [youtube, price, youtube, tv, low, month, including, cloud, dvr, skip, adding, major, program, hole, especially, compared, traditional, cable], [youtube, tv, primarily, based, deals, four, big, broadcast, network, cbs, fox, abc, nbc], [means, youtube, getting, access, content, network, affiliate, cable, channels], [around, total, including, espn, fox, news, msnbc], [youtube, struck, deals, marquee, cable, network, like, turner, discovery, viacom, amc], [means, cnn, tbs, tnt, history, amc, comedy, central, hbo], [youtube, however, trying, keep, sports, offer, robust], [youtube, tv, including, major, sports, network, like, espn, region, sports, network, like, fox, sports, network, comcast, sportsnet, watch, favorite, nba, mlb, team, youtube, said, press, release], [partner, local, tv, station, sports, local, news, based, live], [thinking, youtube, necessarily, convince, established, customer, ditch, month, cable, package, might, convince, young, people, sign, pay, tv, first], [market, youtube, unique, suite, chase, teen, watch, youtube, every, day, according, piper, jaffray], [youtube, end, rainbow, youtube, succeed, nail, tech, getting, legs, competitive, means, streaming, tv, market, instant, goldmine], [margin, early, streaming, tv, service, appear, razor, thin, price, point, youtube, offer, going, change], [crux, opportunity, lies, big, initial, margin, comes, potential, fundamental, shift, tv, adding, work], [stand, tv, network, selling, adding, inventory, show], [early, day, youtube, streaming, tv, service, continue, youtube, selling, minutes, adding, per, hours, service], [change, pacific, crest, analyst, andy, hargreaves, wrote, note, last, month], [googles, vast, superior, data, allow, monetize, adding, inventory, superior, rate, network, wrote], [disparity, allow, capture, greater, share, total, adding, inventory, service], [play, several, years, believe, natural, evolution, success, vmvpd, service, youtube, tv, role, content, supply, adding, selling, split, manage, adding, selling, network, relegated, content, suppliers], [nutshell, youtube, better, selling, adding, network, eventually, charge, selling], [real, potential, profit, taking, huge, bite, billion, global, tv, adding, budget], [watch, marvel, drop, latest, trailer, guardian, galaxy, vol], [look, incredibly]]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "Name: txt2model_stem, dtype: object"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['txt2model_stem'].head(3\n",
    "                         )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
