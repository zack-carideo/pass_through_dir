{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INSTALL TFHUB MODULES ON LOCAL \n",
    " 1. Save the tfhub module down from tfhub.dev to the path you reference for os.environ[\"TFHUB_CACH_DIR\"]\n",
    "    - Note: This module has already been saved for you in zack-carideo public github (https://github.com/zack-carideo/tfhub_embeddings) download the module.\n",
    "    \n",
    " 2. Identify the Hashpath associated with the modules URL\n",
    "  - module_url = \"https://tfhub.dev/google/universal-sentence-encoder/3\"\n",
    "  - hashlib.sha1(module_url.encode(\"utf8\")).hexdigest() \n",
    "\n",
    " 3. Load the module using the hashpath \n",
    "  - mod = tf_hub.Module(\"C:/Users/zjc10/Downloads/tf_hub_cache/42480c3c7f42bf87d36d4c58fc4374b08dae2909/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = \"C:\\\\Users\\\\zjc10\\\\Downloads\\\\tf_hub_cache\"\n",
    "import tensorflow as tf \n",
    "import tensorflow_hub as tf_hub\n",
    "import hashlib\n",
    "import numpy as np\n",
    "\n",
    "# Compute a representation for each message, showing various lengths supported.\n",
    "word = \"Elephant\"\n",
    "sentence = \"I am a sentence for which I would like to get its embedding.\"\n",
    "paragraph = (\n",
    "    \"Universal Sentence Encoder embeddings also support short paragraphs. \"\n",
    "    \"There is no hard limit on how long the paragraph is. Roughly, the longer \"\n",
    "    \"the more 'diluted' the embedding will be.\")\n",
    "messages = [word, sentence, paragraph]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### identify the tfhub hash path associated with the module you want to cache\n",
    " - Each module is saved in a standalone folder named after path hash which can be calculated by the following Python code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'42480c3c7f42bf87d36d4c58fc4374b08dae2909'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/3\"\n",
    "hashlib.sha1(module_url.encode(\"utf8\")).hexdigest() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mod = tf_hub.load(\"C:\\\\Users\\\\zjc10\\\\Downloads\\\\tf_hub_cache\\\\42480c3c7f42bf87d36d4c58fc4374b08dae2909\")\n",
    "mod = tf_hub.Module(\"C:/Users/zjc10/Downloads/tf_hub_cache/42480c3c7f42bf87d36d4c58fc4374b08dae2909/\")\n",
    "\n",
    "#mod = tf_hub.load(module_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "Message: Elephant\n",
      "Embedding size: 512\n",
      "Embedding: [0.04498474299907684, -0.05743396282196045, 0.002211447339504957, ...]\n",
      "\n",
      "Message: I am a sentence for which I would like to get its embedding.\n",
      "Embedding size: 512\n",
      "Embedding: [0.05568019300699234, -0.009607925079762936, 0.006246323697268963, ...]\n",
      "\n",
      "Message: Universal Sentence Encoder embeddings also support short paragraphs. There is no hard limit on how long the paragraph is. Roughly, the longer the more 'diluted' the embedding will be.\n",
      "Embedding size: 512\n",
      "Embedding: [0.038749419152736664, 0.0765201672911644, -0.0007945839897729456, ...]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Compute a representation for each message, showing various lengths supported.\n",
    "word = \"Elephant\"\n",
    "sentence = \"I am a sentence for which I would like to get its embedding.\"\n",
    "paragraph = (\n",
    "    \"Universal Sentence Encoder embeddings also support short paragraphs. \"\n",
    "    \"There is no hard limit on how long the paragraph is. Roughly, the longer \"\n",
    "    \"the more 'diluted' the embedding will be.\")\n",
    "messages = [word, sentence, paragraph]\n",
    "\n",
    "# Reduce logging output.\n",
    "#logging.set_verbosity(logging.ERROR)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    message_embeddings = session.run(mod(messages))\n",
    "\n",
    "    for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
    "        print(\"Message: {}\".format(messages[i]))\n",
    "        print(\"Embedding size: {}\".format(len(message_embedding)))\n",
    "        message_embedding_snippet = \", \".join(\n",
    "            (str(x) for x in message_embedding[:3]))\n",
    "        print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
